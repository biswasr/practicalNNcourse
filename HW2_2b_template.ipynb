{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "import load_cifer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Hyper-perparmeter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "keep_probability = 0.7\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Neural Network Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullconn_net(x, keep_prob):\n",
    "    #x=tf.get_variable('x',shape=[x.shape[0],256],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    flat = tf.contrib.layers.flatten(x)\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=2048, activation_fn=tf.nn.elu, weights_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=1024, activation_fn=tf.nn.elu, weights_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    full2 = tf.layers.batch_normalization(full2)   \n",
    "    \n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.elu, weights_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    full3 = tf.nn.dropout(full3, keep_prob)\n",
    "    full3 = tf.layers.batch_normalization(full3)   \n",
    "    \n",
    "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define cost andoptimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/practicalnn_course/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /anaconda2/envs/practicalnn_course/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-e3630451932f>:5: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-e3630451932f>:6: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-151f2d08cd7a>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = fullconn_net(x, keep_prob)\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and testing</h1>\n",
    "<h2>1.Print out validation accuracy after each training poch</h2>\n",
    "<h2>2.Print out training time you spend on each epoch</h2>\n",
    "<h2>3.Print out testing accuracy in the end</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, feature_batch, label_batch, keep_probability):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, valid_features, valid_labels, cost, accuracy):\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "                             keep_prob: 1\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train=pickle.load(open('features_train.pkl','rb'))\n",
    "features_valid=pickle.load(open('features_valid.pkl','rb'))\n",
    "features_test=pickle.load(open('features_test.pkl','rb'))\n",
    "labels_train=pickle.load(open('labels_train.pkl','rb'))\n",
    "labels_valid=pickle.load(open('labels_valid.pkl','rb'))\n",
    "labels_test=pickle.load(open('labels_test.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.4039 Validation Accuracy: 0.140125\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.4774 Validation Accuracy: 0.132375\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.5388 Validation Accuracy: 0.123875\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.4943 Validation Accuracy: 0.126125\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.3789 Validation Accuracy: 0.138250\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1570 Validation Accuracy: 0.139625\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.1838 Validation Accuracy: 0.148625\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.2677 Validation Accuracy: 0.131625\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.2119 Validation Accuracy: 0.129875\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.2281 Validation Accuracy: 0.135125\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.0420 Validation Accuracy: 0.144750\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.0743 Validation Accuracy: 0.151375\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.1122 Validation Accuracy: 0.148000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.0960 Validation Accuracy: 0.140875\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.0760 Validation Accuracy: 0.147625\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9406 Validation Accuracy: 0.140000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.9654 Validation Accuracy: 0.152250\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.9676 Validation Accuracy: 0.147750\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.9633 Validation Accuracy: 0.150125\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.9563 Validation Accuracy: 0.148000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.8111 Validation Accuracy: 0.150375\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.8228 Validation Accuracy: 0.154625\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.8378 Validation Accuracy: 0.150125\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.8347 Validation Accuracy: 0.144625\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.8169 Validation Accuracy: 0.142875\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.6860 Validation Accuracy: 0.150250\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.7068 Validation Accuracy: 0.154500\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.7080 Validation Accuracy: 0.154750\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.6863 Validation Accuracy: 0.143000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.6701 Validation Accuracy: 0.140500\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.5526 Validation Accuracy: 0.147000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.5679 Validation Accuracy: 0.150750\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.5608 Validation Accuracy: 0.152375\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.5427 Validation Accuracy: 0.143625\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.5258 Validation Accuracy: 0.142500\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.4202 Validation Accuracy: 0.144250\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.4383 Validation Accuracy: 0.147375\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.4341 Validation Accuracy: 0.150250\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.4063 Validation Accuracy: 0.138500\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.3886 Validation Accuracy: 0.138000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.3081 Validation Accuracy: 0.144000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.3291 Validation Accuracy: 0.147250\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.3224 Validation Accuracy: 0.149125\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.2880 Validation Accuracy: 0.139375\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.2662 Validation Accuracy: 0.142375\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.2098 Validation Accuracy: 0.147000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.2208 Validation Accuracy: 0.144750\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.2205 Validation Accuracy: 0.148000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.1805 Validation Accuracy: 0.137375\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.1689 Validation Accuracy: 0.138500\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "# Initializing the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(epochs):\n",
    "    # Loop over all batches\n",
    "    n_batches = 5\n",
    "    for i in range(n_batches):\n",
    "        train_neural_network(sess, optimizer, features_train[i], labels_train[i].toarray(), keep_probability)\n",
    "\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, i+1), end='')\n",
    "        print_stats(sess, features_train[i], labels_train[i].toarray(), features_valid[i], labels_valid[i].toarray(), cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=sess.run(accuracy,feed_dict={x: features_test,y: labels_test.toarray(),keep_prob: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1439\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
