{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "import load_cifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LeNet-5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet_5(x):\n",
    "\n",
    "    # Layer 1 : Convolutional Layer. Input = 32x32x3, Output = 28x28x3.\n",
    "    conv1_w = tf.Variable(tf.truncated_normal(shape = [5,5,3,6],mean = 0, stddev = 0.1))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1 = tf.nn.conv2d(x,conv1_w, strides = [1,1,1,1], padding = 'VALID') + conv1_b \n",
    "    # TODO: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Pooling Layer. Input = 28x28x3. Output = 14x14x6.\n",
    "    pool_1 = tf.nn.max_pool(conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "\n",
    "\n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_w = tf.Variable(tf.truncated_normal(shape = [5,5,6,16], mean = 0, stddev = 0.1))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [1,1,1,1], padding = 'VALID') + conv2_b\n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    pool_2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "\n",
    "\n",
    "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc1 = tf.contrib.layers.flatten(pool_2)\n",
    "\n",
    "\n",
    "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_w = tf.Variable(tf.truncated_normal(shape = (400,120), mean = 0, stddev = 0.1))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
    "\n",
    "    # TODO: Activation.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_w = tf.Variable(tf.truncated_normal(shape = (120,84), mean = 0, stddev = 0.1))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
    "    # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_w = tf.Variable(tf.truncated_normal(shape = (84,10), mean = 0 , stddev = 0.1))\n",
    "    fc3_b = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/practicalnn_course/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-015bff715df2>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = LeNet_5(x)\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training, validation and testing</h1>\n",
    "<h2>Train your model only 10 epochs.</h2>\n",
    "<h2>1.Print out validation accuracy after each training epoch</h2>\n",
    "<h2>2.Print out training time for each training epoch</h2>\n",
    "<h2>3.Print out testing accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, valid_features, valid_labels, cost, accuracy):\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train=pickle.load(open('features_train.pkl','rb'))\n",
    "features_valid=pickle.load(open('features_valid.pkl','rb'))\n",
    "features_test=pickle.load(open('features_test.pkl','rb'))\n",
    "labels_train=pickle.load(open('labels_train.pkl','rb'))\n",
    "labels_valid=pickle.load(open('labels_valid.pkl','rb'))\n",
    "labels_test=pickle.load(open('labels_test.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2981 Validation Accuracy: 0.101125\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2989 Validation Accuracy: 0.109250\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2959 Validation Accuracy: 0.121000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.2956 Validation Accuracy: 0.120000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.2939 Validation Accuracy: 0.123125\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2878 Validation Accuracy: 0.130375\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.2896 Validation Accuracy: 0.125125\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.2838 Validation Accuracy: 0.141125\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.2842 Validation Accuracy: 0.126375\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.2799 Validation Accuracy: 0.133625\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2737 Validation Accuracy: 0.139250\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.2784 Validation Accuracy: 0.133625\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.2696 Validation Accuracy: 0.149125\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.2717 Validation Accuracy: 0.133875\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.2661 Validation Accuracy: 0.137625\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.2592 Validation Accuracy: 0.145125\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.2684 Validation Accuracy: 0.135625\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.2556 Validation Accuracy: 0.147500\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.2592 Validation Accuracy: 0.144625\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.2557 Validation Accuracy: 0.146375\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.2479 Validation Accuracy: 0.147125\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     2.2592 Validation Accuracy: 0.139000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     2.2441 Validation Accuracy: 0.152500\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     2.2467 Validation Accuracy: 0.149875\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     2.2447 Validation Accuracy: 0.148250\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.2371 Validation Accuracy: 0.151750\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     2.2492 Validation Accuracy: 0.146750\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     2.2334 Validation Accuracy: 0.152750\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     2.2355 Validation Accuracy: 0.154750\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     2.2326 Validation Accuracy: 0.147125\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.2273 Validation Accuracy: 0.147875\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     2.2384 Validation Accuracy: 0.150750\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     2.2231 Validation Accuracy: 0.156625\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     2.2239 Validation Accuracy: 0.153250\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     2.2216 Validation Accuracy: 0.150625\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.2172 Validation Accuracy: 0.149375\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     2.2267 Validation Accuracy: 0.154500\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     2.2127 Validation Accuracy: 0.158250\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     2.2132 Validation Accuracy: 0.155500\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     2.2103 Validation Accuracy: 0.151875\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.2058 Validation Accuracy: 0.151000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     2.2159 Validation Accuracy: 0.162500\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     2.2024 Validation Accuracy: 0.165250\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     2.2031 Validation Accuracy: 0.163125\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     2.1981 Validation Accuracy: 0.156000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.1942 Validation Accuracy: 0.158000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     2.2052 Validation Accuracy: 0.165500\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     2.1910 Validation Accuracy: 0.170375\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     2.1931 Validation Accuracy: 0.165250\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     2.1858 Validation Accuracy: 0.159875\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "# Initializing the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(epochs):\n",
    "    # Loop over all batches\n",
    "    n_batches = 5\n",
    "    for i in range(n_batches):\n",
    "        train_neural_network(sess, optimizer, features_train[i], labels_train[i].toarray())\n",
    "\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, i+1), end='')\n",
    "        print_stats(sess, features_train[i], labels_train[i].toarray(), features_valid[i], labels_valid[i].toarray(), cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1622\n"
     ]
    }
   ],
   "source": [
    "result=sess.run(accuracy,feed_dict={x: features_test,y: labels_test.toarray()})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
